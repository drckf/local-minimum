<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Charles Fisher">
<meta name="dcterms.date" content="2025-04-06">

<title>What is Weighted Linear Attention? – Local Minimum</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-6bd9cfa162949bde0a231f530c97869d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Local Minimum</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../license.html"> 
<span class="menu-text">License</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/drckf"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">What is Weighted Linear Attention?</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">review</div>
                <div class="quarto-category">machine-learning</div>
                <div class="quarto-category">attention</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Charles Fisher </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 6, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="tldr" class="level2">
<h2 class="anchored" data-anchor-id="tldr">TLDR</h2>
<p>Weighted linear attention is a type of key-value associative memory defined by the matrix <span class="math display">\[
\begin{equation}
J = \sum_{l'} w_{l'} \, \vec{v}_{l'} \, \vec{k}_{l'}^T
\end{equation}
\]</span> and an associative recall formula <span class="math inline">\(\tilde{v}_l = J \vec{q}_l\)</span> that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows.</p>
</section>
<section id="softmax-attention" class="level2">
<h2 class="anchored" data-anchor-id="softmax-attention">Softmax Attention</h2>
<p>The softmax attention module was a key innovation in the development of transformer models that are commonly used for natural language processing and other applications in machine learning <span class="citation" data-cites="vaswani2017attention">(<a href="#ref-vaswani2017attention" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. At a high level, attention leverages the idea that the meaning of a word in a document could change depending on the other words in the document. Therefore, in order to determine the meaning of a given word (or a component of a word called a token) we need to compare it to the rest of the words in the document.</p>
<p>Let <span class="math inline">\(\vec{x}_l \in \mathbb{R}^n\)</span> for <span class="math inline">\(l = 1, \ldots, L\)</span> be a sequence of tokens, <span class="math inline">\(W_Q\)</span> and <span class="math inline">\(W_K\)</span> be <span class="math inline">\(d_k \times n\)</span> matrices, and <span class="math inline">\(W_V\)</span> be a <span class="math inline">\(d_v \times n\)</span> matrix. The queries, keys, and values are given by <span class="math display">\[
\begin{align}
\vec{q}_l &amp;= W_Q \, \vec{x}_l \in \mathbb{R}^{d_k} \\
\vec{k}_l &amp;= W_K \, \vec{x}_l \in \mathbb{R}^{d_k} \\
\vec{v}_l &amp;= W_V \, \vec{x}_l \in \mathbb{R}^{d_v}
\end{align}
\]</span> The softmax attention module defines an <span class="math inline">\(L \times L\)</span> matrix <span class="math inline">\(\mathcal{A}\)</span> with elements <span class="math display">\[
\mathcal{A}_{l,l'} = \frac{e^{ d_k^{-1/2} \vec{q}_l^T \vec{k}_{l'} }}{ \sum_{l''} e^{d_k^{-1/2} \vec{q}_l^T \vec{k}_{l''} }}
\]</span> that describes how query <span class="math inline">\(l\)</span> attends to key <span class="math inline">\(l'\)</span>. Thus, the value retrived for query <span class="math inline">\(l\)</span> is <span class="math display">\[
\tilde{v}_l = \sum_{l'} \mathcal{A}_{l,l'} \vec{v}_{l'}
\]</span> Of course, there are many variations of this general module that have been introduced in the last few years, but I’m not going to get into those because the original formulation has the necessary elements for this introduction.</p>
<p>Softmax attention is relatively easy to implement in parallel because it is stateless. This means that it efficiently utilizes the capabilities of modern GPUs, but it also requires a large amount of memory and computational resources because it involves computing an <span class="math inline">\(L \times L\)</span> matrix. Thus, softmax attention uses resources that are quadratic in the length of the context window. Although one can use a variety of tricks to mitigate this problem, it nevertheless represents a fundamental limitation to the length of sequences that transformers are able to process.</p>
</section>
<section id="weighted-linear-attention" class="level2">
<h2 class="anchored" data-anchor-id="weighted-linear-attention">Weighted Linear Attention</h2>
<p>In the last few years, there has been renewed interest in stateful architectures for modeling sequential data such as language or time series <span class="citation" data-cites="wang2025test aksenov2024linear schlag2021linear yang2025parallelizing katharopoulos2020transformers peng2021random beck2025xlstm qin2022cosformer kasai2021finetuning zhang2024hedgehog chen2024dijiang sun2023retentive orvieto2023resurrecting katsch2023gateloop de2024griffin qin2024hgrn2 peng2024eagle yang2023gated gu2023mamba dao2024transformers liu2024longhorn sun2024learning yang2024gated behrouz2024titans">(<a href="#ref-wang2025test" role="doc-biblioref">Wang, Shi, and Fox 2025</a>; <a href="#ref-aksenov2024linear" role="doc-biblioref">Aksenov et al. 2024</a>; <a href="#ref-schlag2021linear" role="doc-biblioref">Schlag, Irie, and Schmidhuber 2021</a>; <a href="#ref-yang2025parallelizing" role="doc-biblioref">Songlin Yang et al. 2025</a>, <a href="#ref-yang2023gated" role="doc-biblioref">2023</a>; <a href="#ref-katharopoulos2020transformers" role="doc-biblioref">Katharopoulos et al. 2020</a>; <a href="#ref-peng2021random" role="doc-biblioref">H. Peng et al. 2021</a>; <a href="#ref-beck2025xlstm" role="doc-biblioref">Beck et al. 2025</a>; <a href="#ref-qin2022cosformer" role="doc-biblioref">Qin et al. 2022</a>, <a href="#ref-qin2024hgrn2" role="doc-biblioref">2024</a>; <a href="#ref-kasai2021finetuning" role="doc-biblioref">Kasai et al. 2021</a>; <a href="#ref-zhang2024hedgehog" role="doc-biblioref">Zhang et al. 2024</a>; <a href="#ref-chen2024dijiang" role="doc-biblioref">Chen et al. 2024</a>; <a href="#ref-sun2023retentive" role="doc-biblioref">Yutao Sun et al. 2023</a>; <a href="#ref-orvieto2023resurrecting" role="doc-biblioref">Orvieto et al. 2023</a>; <a href="#ref-katsch2023gateloop" role="doc-biblioref">Katsch 2023</a>; <a href="#ref-de2024griffin" role="doc-biblioref">De et al. 2024</a>; <a href="#ref-peng2024eagle" role="doc-biblioref">B. Peng et al. 2024</a>; <a href="#ref-gu2023mamba" role="doc-biblioref">Gu and Dao 2023</a>; <a href="#ref-dao2024transformers" role="doc-biblioref">Dao and Gu 2024</a>; <a href="#ref-liu2024longhorn" role="doc-biblioref">Liu et al. 2024</a>; <a href="#ref-sun2024learning" role="doc-biblioref">Yu Sun et al. 2024</a>; <a href="#ref-yang2024gated" role="doc-biblioref">Songlin Yang, Kautz, and Hatamizadeh 2024</a>; <a href="#ref-behrouz2024titans" role="doc-biblioref">Behrouz, Zhong, and Mirrokni 2024</a>)</span>. Stateful models are a more memory and compute efficient alternative to stateless models like transformers because they scale linearly with the length of the sequence in contrast to the quadratic scaling of transformers. In principle, stateful language models could have much cheaper inference costs and would be able to operate over essentially infinite context windows.</p>
<p>Modern stateful sequence models typically use a key-value associative memory as an alternative to the softmax attention heads in transformers. A key-value associative memory is a <span class="math inline">\(d_v \times d_k\)</span> matrix <span class="math inline">\(J_l\)</span> that is typically, but not necessarily, updated through a recurrent relation such as <span class="math display">\[
\begin{equation}
J_l = \omega_{f,l} \, J_{l-1} + \omega_{i,l} \, \vec{v}_l \, \vec{k}_l^T
\end{equation}
\]</span> where <span class="math inline">\(\omega_{f,l} \geq 0\)</span> is a forget gate and <span class="math inline">\(\omega_{i,l} \geq 0\)</span> is an input gate. The input and forget gates could be constants, functions of the current state, or could even be functions of all previous states in the sequence. A value is retrieved from the memory by multiplication with a query vector <span class="math inline">\(\tilde{v}_l = J \, \vec{q}_l\)</span>, where I am denoting the retrieved value with a tilde to emphasize that <span class="math inline">\(\tilde{v}_i\)</span> will generally not equal <span class="math inline">\(\vec{v}_i\)</span> unless the memory has perfect recall. See <a href="#fig-linear-attention">Figure 1</a> below for examples of different linear attention mechanisms.</p>
<p>It’s trivial to see that an associative memory matrix trained through this type of recurrent update rule will take the form <span class="math display">\[
\begin{equation}
J = \sum_{l'} w_{l'} \, \vec{v}_{l'} \, \vec{k}_{l'}^T
\end{equation}
\]</span> where <span class="math inline">\(w_{l'} \geq 0\)</span> is the weight assigned to token <span class="math inline">\(l'\)</span> in the memory. I will refer to key-value associative memories with this structure as “weighted linear attention” modules.</p>
<p>To illustrate the connection between the recurrent update and the weighted linear attention form, consider unrolling the recurrence. Starting with <span class="math display">\[
J_l = \omega_{f,l} J_{l-1} + \omega_{i,l}\, \vec{v}_l\, \vec{k}_l^T,
\]</span> recursively substituting <span class="math inline">\(J_{l-1}\)</span> gives <span class="math display">\[
J_l = \sum_{l'=1}^{l} \left( \prod_{l''=l'+1}^{l} \omega_{f,l''} \right) \omega_{i,l'}\, \vec{v}_{l'}\, \vec{k}_{l'}^T.
\]</span> This shows that the associative memory <span class="math inline">\(J_l\)</span> is a weighted sum of outer products of values and keys, where each token’s contribution is scaled by a weight <span class="math inline">\(w_{l'} = \left( \prod_{l''=l'+1}^{l} \omega_{f,l''} \right) \omega_{i,l'}\)</span>. These weights capture how much past tokens are retained in memory, clarifying how stateful, linear attention aggregates information over time.</p>
<p>Although much of the practical interest in linear attention architectures is the ability to define them through recurrent update rules, it’s not clear that these recurrent update rules optimize their memory capacity. That is, there may be alternative ways to learn the weights that lead to better performing associative memories, at least in theory. I’ll explore this in a series of upcoming blog posts focused on a surprising connection between weighted linear attention modules and ecological systems.</p>
<p>Specifically, I will show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. In a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention. Follow along with the next post in this series <a href="../../posts/weighted-linear-attention-is-lotka-volterra/">Weighted Linear Attention is Lotka-Volterra Dynamics</a>.</p>
<hr>
<p>The following table from Yang et al (2024) highlights a number of models that use various types of linear attention modules and recurrent update rules:</p>
<div id="fig-linear-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-linear-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="neurips24_poster_deltanet.jpg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-linear-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Types of Recurrent Updates for Linear Attention. (Source: <span class="citation" data-cites="yang2024parallelizing">S. Yang et al. (<a href="#ref-yang2024parallelizing" role="doc-biblioref">2024</a>)</span>)
</figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-aksenov2024linear" class="csl-entry" role="listitem">
Aksenov, Yaroslav, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, and Daniil Gavrilov. 2024. <span>“Linear Transformers with Learnable Kernel Functions Are Better in-Context Models.”</span> <em>arXiv Preprint arXiv:2402.10644</em>.
</div>
<div id="ref-beck2025xlstm" class="csl-entry" role="listitem">
Beck, Maximilian, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2025. <span>“Xlstm: Extended Long Short-Term Memory.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 107547–603.
</div>
<div id="ref-behrouz2024titans" class="csl-entry" role="listitem">
Behrouz, Ali, Peilin Zhong, and Vahab Mirrokni. 2024. <span>“Titans: Learning to Memorize at Test Time.”</span> <em>arXiv Preprint arXiv:2501.00663</em>.
</div>
<div id="ref-chen2024dijiang" class="csl-entry" role="listitem">
Chen, Hanting, Zhicheng Liu, Xutao Wang, Yuchuan Tian, and Yunhe Wang. 2024. <span>“Dijiang: Efficient Large Language Models Through Compact Kernelization.”</span> <em>arXiv Preprint arXiv:2403.19928</em>.
</div>
<div id="ref-dao2024transformers" class="csl-entry" role="listitem">
Dao, Tri, and Albert Gu. 2024. <span>“Transformers Are Ssms: Generalized Models and Efficient Algorithms Through Structured State Space Duality.”</span> <em>arXiv Preprint arXiv:2405.21060</em>.
</div>
<div id="ref-de2024griffin" class="csl-entry" role="listitem">
De, Soham, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, et al. 2024. <span>“Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models.”</span> <em>arXiv Preprint arXiv:2402.19427</em>.
</div>
<div id="ref-gu2023mamba" class="csl-entry" role="listitem">
Gu, Albert, and Tri Dao. 2023. <span>“Mamba: Linear-Time Sequence Modeling with Selective State Spaces.”</span> <em>arXiv Preprint arXiv:2312.00752</em>.
</div>
<div id="ref-kasai2021finetuning" class="csl-entry" role="listitem">
Kasai, Jungo, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. 2021. <span>“Finetuning Pretrained Transformers into Rnns.”</span> <em>arXiv Preprint arXiv:2103.13076</em>.
</div>
<div id="ref-katharopoulos2020transformers" class="csl-entry" role="listitem">
Katharopoulos, Angelos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. 2020. <span>“Transformers Are Rnns: Fast Autoregressive Transformers with Linear Attention.”</span> In <em>International Conference on Machine Learning</em>, 5156–65. PMLR.
</div>
<div id="ref-katsch2023gateloop" class="csl-entry" role="listitem">
Katsch, Tobias. 2023. <span>“Gateloop: Fully Data-Controlled Linear Recurrence for Sequence Modeling.”</span> <em>arXiv Preprint arXiv:2311.01927</em>.
</div>
<div id="ref-liu2024longhorn" class="csl-entry" role="listitem">
Liu, Bo, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, and Qiang Liu. 2024. <span>“Longhorn: State Space Models Are Amortized Online Learners.”</span> <em>arXiv Preprint arXiv:2407.14207</em>.
</div>
<div id="ref-orvieto2023resurrecting" class="csl-entry" role="listitem">
Orvieto, Antonio, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. <span>“Resurrecting Recurrent Neural Networks for Long Sequences.”</span> In <em>International Conference on Machine Learning</em>, 26670–98. PMLR.
</div>
<div id="ref-peng2024eagle" class="csl-entry" role="listitem">
Peng, Bo, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, et al. 2024. <span>“Eagle and Finch: Rwkv with Matrix-Valued States and Dynamic Recurrence.”</span> <em>arXiv Preprint arXiv:2404.05892</em> 3.
</div>
<div id="ref-peng2021random" class="csl-entry" role="listitem">
Peng, Hao, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng Kong. 2021. <span>“Random Feature Attention.”</span> <em>arXiv Preprint arXiv:2103.02143</em>.
</div>
<div id="ref-qin2022cosformer" class="csl-entry" role="listitem">
Qin, Zhen, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. <span>“Cosformer: Rethinking Softmax in Attention.”</span> <em>arXiv Preprint arXiv:2202.08791</em>.
</div>
<div id="ref-qin2024hgrn2" class="csl-entry" role="listitem">
Qin, Zhen, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. 2024. <span>“Hgrn2: Gated Linear Rnns with State Expansion.”</span> <em>arXiv Preprint arXiv:2404.07904</em>.
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
Schlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. <span>“Linear Transformers Are Secretly Fast Weight Programmers.”</span> In <em>International Conference on Machine Learning</em>, 9355–66. PMLR.
</div>
<div id="ref-sun2024learning" class="csl-entry" role="listitem">
Sun, Yu, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, et al. 2024. <span>“Learning to (Learn at Test Time): Rnns with Expressive Hidden States.”</span> <em>arXiv Preprint arXiv:2407.04620</em>.
</div>
<div id="ref-sun2023retentive" class="csl-entry" role="listitem">
Sun, Yutao, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. 2023. <span>“Retentive Network: A Successor to Transformer for Large Language Models.”</span> <em>arXiv Preprint arXiv:2307.08621</em>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-wang2025test" class="csl-entry" role="listitem">
Wang, Ke Alexander, Jiaxin Shi, and Emily B Fox. 2025. <span>“Test-Time Regression: A Unifying Framework for Designing Sequence Models with Associative Memory.”</span> <em>arXiv Preprint arXiv:2501.12352</em>.
</div>
<div id="ref-yang2024gated" class="csl-entry" role="listitem">
Yang, Songlin, Jan Kautz, and Ali Hatamizadeh. 2024. <span>“Gated Delta Networks: Improving Mamba2 with Delta Rule.”</span> <em>arXiv Preprint arXiv:2412.06464</em>.
</div>
<div id="ref-yang2023gated" class="csl-entry" role="listitem">
Yang, Songlin, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. 2023. <span>“Gated Linear Attention Transformers with Hardware-Efficient Training.”</span> <em>arXiv Preprint arXiv:2312.06635</em>.
</div>
<div id="ref-yang2025parallelizing" class="csl-entry" role="listitem">
Yang, Songlin, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. 2025. <span>“Parallelizing Linear Transformers with the Delta Rule over Sequence Length.”</span> <em>Advances in Neural Information Processing Systems</em> 37: 115491–522.
</div>
<div id="ref-yang2024parallelizing" class="csl-entry" role="listitem">
Yang, S., B. Wang, Y. Zhang, Y. Shen, and Y. Kim. 2024. <span>“Parallelizing Linear Transformers with the Delta Rule over Sequence Length.”</span> <em>NeurIPS</em>.
</div>
<div id="ref-zhang2024hedgehog" class="csl-entry" role="listitem">
Zhang, Michael, Kush Bhatia, Hermann Kumbong, and Christopher Ré. 2024. <span>“The Hedgehog &amp; the Porcupine: Expressive Linear Attentions with Softmax Mimicry.”</span> <em>arXiv Preprint arXiv:2402.04347</em>.
</div>
</div></section></div></main> <!-- /main -->
<hr style="margin: 2rem 0; border: 0; border-top: 2px solid #ccc;">

<h2>Subscribe to Updates</h2>

<p>Enter your email below if you want to subscribe to be automatically notified of new posts.</p>

<form action="https://assets.mailerlite.com/jsonp/1441877/forms/151220238201914872/subscribe" method="post" target="_blank" style="max-width: 500px; margin-bottom: 30px;">
  <div style="display: flex; margin-bottom: 10px;">
    <input type="email" name="fields[email]" placeholder="Your email address" required="" style="flex-grow: 1; padding: 8px; margin-right: 10px; border: 1px solid #ccc; border-radius: 4px;">
    <button type="submit" style="background-color: #1677be; color: white; border: none; padding: 8px 16px; border-radius: 4px; cursor: pointer;">
      Subscribe
    </button>
  </div>
  <p style="font-size: 0.8em; color: #666;">
    We respect your privacy. Unsubscribe at any time.
  </p>
  <input type="hidden" name="ml-submit" value="1">
  <input type="hidden" name="anticsrf" value="true">
</form>

<script>
  document.addEventListener('DOMContentLoaded', function() {
    const form = document.querySelector('form[action*="mailerlite"]');
    if (form) {
      form.addEventListener('submit', function(e) {
        e.preventDefault();
        const email = form.querySelector('input[name="fields[email]"]').value;
        const formData = new FormData();
        formData.append('fields[email]', email);
        formData.append('ml-submit', '1');
        formData.append('anticsrf', 'true');
        
        fetch(form.action, {
          method: 'POST',
          body: formData,
          mode: 'no-cors'
        })
        .then(() => {
          // Show success message
          form.innerHTML = '<p style="color: #1677be; font-weight: bold;">Thank you! You\'ve successfully subscribed to Local Minimum updates.</p>';
        });
      });
    }
  });
</script>

<div class="license-footer" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid #eee; font-size: 0.8em; color: #666;">
  © 2025 Charles Fisher. This work is licensed under a <a href="../../license.html">Creative Commons Attribution 4.0 International License</a>.
</div>

<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/drckf\.github\.io\/local-minimum\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="drckf/local-minimum" data-repo-id="R_kgDOOVGEfA" data-category="Announcements" data-category-id="DIC_kwDOOVGEfM4Co189" data-mapping="pathname" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="light">
</div> <!-- /content -->




</body></html>