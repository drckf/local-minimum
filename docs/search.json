[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "license.html#creative-commons-attribution-4.0-international-license",
    "href": "license.html#creative-commons-attribution-4.0-international-license",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "index.html#welcome-to-the-local-minimum",
    "href": "index.html#welcome-to-the-local-minimum",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html",
    "href": "posts/weighted-linear-attention/index.html",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#tldr",
    "href": "posts/weighted-linear-attention/index.html#tldr",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#softmax-attention",
    "href": "posts/weighted-linear-attention/index.html#softmax-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Softmax Attention",
    "text": "Softmax Attention\nThe softmax attention module was a key innovation in the development of transformer models that are commonly used for natural language processing and other applications in machine learning (Vaswani et al. 2017). At a high level, attention leverages the idea that the meaning of a word in a document could change depending on the other words in the document. Therefore, in order to determine the meaning of a given word (or a component of a word called a token) we need to compare it to the rest of the words in the document.\nLet \\(\\vec{x}_l \\in \\mathbb{R}^n\\) for \\(l = 1, \\ldots, L\\) be a sequence of tokens, \\(W_Q\\) and \\(W_K\\) be \\(d_k \\times n\\) matrices, and \\(W_V\\) be a \\(d_v \\times n\\) matrix. The queries, keys, and values are given by \\[\n\\begin{align}\n\\vec{q}_l &= W_Q \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{k}_l &= W_K \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{v}_l &= W_V \\, \\vec{x}_l \\in \\mathbb{R}^{d_v}\n\\end{align}\n\\] The softmax attention module defines an \\(L \\times L\\) matrix \\(\\mathcal{A}\\) with elements \\[\n\\mathcal{A}_{l,l'} = \\frac{e^{ d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l'} }}{ \\sum_{l''} e^{d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l''} }}\n\\] that describes how query \\(l\\) attends to key \\(l'\\). Thus, the value retrived for query \\(l\\) is \\[\n\\tilde{v}_l = \\sum_{l'} \\mathcal{A}_{l,l'} \\vec{v}_{l'}\n\\] Of course, there are many variations of this general module that have been introduced in the last few years, but I’m not going to get into those because the original formulation has the necessary elements for this introduction.\nSoftmax attention is relatively easy to implement in parallel because it is stateless. This means that it efficiently utilizes the capabilities of modern GPUs, but it also requires a large amount of memory and computational resources because it involves computing an \\(L \\times L\\) matrix. Thus, softmax attention uses resources that are quadratic in the length of the context window. Although one can use a variety of tricks to mitigate this problem, it nevertheless represents a fundamental limitation to the length of sequences that transformers are able to process."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "href": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Weighted Linear Attention",
    "text": "Weighted Linear Attention\nIn the last few years, there has been renewed interest in stateful architectures for modeling sequential data such as language or time series (Wang, Shi, and Fox 2025; Aksenov et al. 2024; Schlag, Irie, and Schmidhuber 2021; Songlin Yang et al. 2025, 2023; Katharopoulos et al. 2020; H. Peng et al. 2021; Beck et al. 2025; Qin et al. 2022, 2024; Kasai et al. 2021; Zhang et al. 2024; Chen et al. 2024; Yutao Sun et al. 2023; Orvieto et al. 2023; Katsch 2023; De et al. 2024; B. Peng et al. 2024; Gu and Dao 2023; Dao and Gu 2024; Liu et al. 2024; Yu Sun et al. 2024; Songlin Yang, Kautz, and Hatamizadeh 2024; Behrouz, Zhong, and Mirrokni 2024). Stateful models are a more memory and compute efficient alternative to stateless models like transformers because they scale linearly with the length of the sequence in contrast to the quadratic scaling of transformers. In principle, stateful language models could have much cheaper inference costs and would be able to operate over essentially infinite context windows.\nModern stateful sequence models typically use a key-value associative memory as an alternative to the softmax attention heads in transformers. A key-value associative memory is a \\(d_v \\times d_k\\) matrix \\(J_l\\) that is typically, but not necessarily, updated through a recurrent relation such as \\[\n\\begin{equation}\nJ_l = \\omega_{f,l} \\, J_{l-1} + \\omega_{i,l} \\, \\vec{v}_l \\, \\vec{k}_l^T\n\\end{equation}\n\\] where \\(\\omega_{f,l} \\geq 0\\) is a forget gate and \\(\\omega_{i,l} \\geq 0\\) is an input gate. The input and forget gates could be constants, functions of the current state, or could even be functions of all previous states in the sequence. A value is retrieved from the memory by multiplication with a query vector \\(\\tilde{v}_l = J \\, \\vec{q}_l\\), where I am denoting the retrieved value with a tilde to emphasize that \\(\\tilde{v}_i\\) will generally not equal \\(\\vec{v}_i\\) unless the memory has perfect recall. See Figure 1 below for examples of different linear attention mechanisms.\nIt’s trivial to see that an associative memory matrix trained through this type of recurrent update rule will take the form \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] where \\(w_{l'} \\geq 0\\) is the weight assigned to token \\(l'\\) in the memory. I will refer to key-value associative memories with this structure as “weighted linear attention” modules.\nTo illustrate the connection between the recurrent update and the weighted linear attention form, consider unrolling the recurrence. Starting with \\[\nJ_l = \\omega_{f,l} J_{l-1} + \\omega_{i,l}\\, \\vec{v}_l\\, \\vec{k}_l^T,\n\\] recursively substituting \\(J_{l-1}\\) gives \\[\nJ_l = \\sum_{l'=1}^{l} \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\, \\vec{v}_{l'}\\, \\vec{k}_{l'}^T.\n\\] This shows that the associative memory \\(J_l\\) is a weighted sum of outer products of values and keys, where each token’s contribution is scaled by a weight \\(w_{l'} = \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\). These weights capture how much past tokens are retained in memory, clarifying how stateful, linear attention aggregates information over time.\nAlthough much of the practical interest in linear attention architectures is the ability to define them through recurrent update rules, it’s not clear that these recurrent update rules optimize their memory capacity. That is, there may be alternative ways to learn the weights that lead to better performing associative memories, at least in theory. I’ll explore this in a series of upcoming blog posts focused on a surprising connection between weighted linear attention modules and ecological systems.\nSpecifically, I will show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. In a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention. Follow along with the next post in this series Weighted Linear Attention is Lotka-Volterra Dynamics.\n\nThe following table from Yang et al (2024) highlights a number of models that use various types of linear attention modules and recurrent update rules:\n\n\n\n\n\n\nFigure 1: Types of Recurrent Updates for Linear Attention. (Source: S. Yang et al. (2024))\n\n\n\n\n\n© 2025 Charles Fisher. This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nTechnology is often developed out in the open, through open source projects that allow one to follow along with the development process in addition to consuming the final product. But scientific research is usually done behind closed doors, for reasons I don’t entirely understand. The thinking process that leads to a paper isn’t shared, except among a few collaborators. This blog is my attempt at truly open scientific research.\nThe blog is called “Local Minimum” because many of the ideas I share here will be early and immature. It’s a nod to the way can systems can get stuck in a local minimum of an energy landscape.\n\n\nAbout the author\nCharles Fisher is typically interested in research at the intersection of physics, machine learning, and biology. He has a PhD in biophysics from Harvard University, a BS in biophysics from the University of Michigan, and did postdoctoral research in biophysics at Boston University and Ecole Normale Superieure in Paris. Following his time in academia, he moved to industry where he worked at Pfizer and Leap Motion before starting Unlearn.AI. He was CEO of Unlearn for nearly 8 years before stepping back to a board role so that he could get back to his roots and dive into research again. Hence, this blog.\n\n\nAcknowledgments\nI’m often discussing my research with Pankaj Mehta and his group at Boston University, Austin Huang, Anton Loukianov, and others."
  }
]