[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "license.html#creative-commons-attribution-4.0-international-license",
    "href": "license.html#creative-commons-attribution-4.0-international-license",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "index.html#welcome-to-the-local-minimum",
    "href": "index.html#welcome-to-the-local-minimum",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html",
    "href": "posts/weighted-linear-attention/index.html",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#tldr",
    "href": "posts/weighted-linear-attention/index.html#tldr",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#softmax-attention",
    "href": "posts/weighted-linear-attention/index.html#softmax-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Softmax Attention",
    "text": "Softmax Attention\nThe softmax attention module was a key innovation in the development of transformer models that are commonly used for natural language processing and other applications in machine learning (Vaswani et al. 2017). At a high level, attention leverages the idea that the meaning of a word in a document could change depending on the other words in the document. Therefore, in order to determine the meaning of a given word (or a component of a word called a token) we need to compare it to the rest of the words in the document.\nLet \\(\\vec{x}_l \\in \\mathbb{R}^n\\) for \\(l = 1, \\ldots, L\\) be a sequence of tokens, \\(W_Q\\) and \\(W_K\\) be \\(d_k \\times n\\) matrices, and \\(W_V\\) be a \\(d_v \\times n\\) matrix. The queries, keys, and values are given by \\[\n\\begin{align}\n\\vec{q}_l &= W_Q \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{k}_l &= W_K \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{v}_l &= W_V \\, \\vec{x}_l \\in \\mathbb{R}^{d_v}\n\\end{align}\n\\] The softmax attention module defines an \\(L \\times L\\) matrix \\(\\mathcal{A}\\) with elements \\[\n\\mathcal{A}_{l,l'} = \\frac{e^{ d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l'} }}{ \\sum_{l''} e^{d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l''} }}\n\\] that describes how query \\(l\\) attends to key \\(l'\\). Thus, the value retrived for query \\(l\\) is \\[\n\\tilde{v}_l = \\sum_{l'} \\mathcal{A}_{l,l'} \\vec{v}_{l'}\n\\] Of course, there are many variations of this general module that have been introduced in the last few years, but I’m not going to get into those because the original formulation has the necessary elements for this introduction.\nSoftmax attention is relatively easy to implement in parallel because it is stateless. This means that it efficiently utilizes the capabilities of modern GPUs, but it also requires a large amount of memory and computational resources because it involves computing an \\(L \\times L\\) matrix. Thus, softmax attention uses resources that are quadratic in the length of the context window. Although one can use a variety of tricks to mitigate this problem, it nevertheless represents a fundamental limitation to the length of sequences that transformers are able to process."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "href": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Weighted Linear Attention",
    "text": "Weighted Linear Attention\nIn the last few years, there has been renewed interest in stateful architectures for modeling sequential data such as language or time series (Wang, Shi, and Fox 2025; Aksenov et al. 2024; Schlag, Irie, and Schmidhuber 2021; Songlin Yang et al. 2025, 2023; Katharopoulos et al. 2020; H. Peng et al. 2021; Beck et al. 2025; Qin et al. 2022, 2024; Kasai et al. 2021; Zhang et al. 2024; Chen et al. 2024; Yutao Sun et al. 2023; Orvieto et al. 2023; Katsch 2023; De et al. 2024; B. Peng et al. 2024; Gu and Dao 2023; Dao and Gu 2024; Liu et al. 2024; Yu Sun et al. 2024; Songlin Yang, Kautz, and Hatamizadeh 2024; Behrouz, Zhong, and Mirrokni 2024). Stateful models are a more memory and compute efficient alternative to stateless models like transformers because they scale linearly with the length of the sequence in contrast to the quadratic scaling of transformers. In principle, stateful language models could have much cheaper inference costs and would be able to operate over essentially infinite context windows.\nModern stateful sequence models typically use a key-value associative memory as an alternative to the softmax attention heads in transformers. A key-value associative memory is a \\(d_v \\times d_k\\) matrix \\(J_l\\) that is typically, but not necessarily, updated through a recurrent relation such as \\[\n\\begin{equation}\nJ_l = \\omega_{f,l} \\, J_{l-1} + \\omega_{i,l} \\, \\vec{v}_l \\, \\vec{k}_l^T\n\\end{equation}\n\\] where \\(\\omega_{f,l} \\geq 0\\) is a forget gate and \\(\\omega_{i,l} \\geq 0\\) is an input gate. The input and forget gates could be constants, functions of the current state, or could even be functions of all previous states in the sequence. A value is retrieved from the memory by multiplication with a query vector \\(\\tilde{v}_l = J \\, \\vec{q}_l\\), where I am denoting the retrieved value with a tilde to emphasize that \\(\\tilde{v}_i\\) will generally not equal \\(\\vec{v}_i\\) unless the memory has perfect recall. See Figure 1 below for examples of different linear attention mechanisms.\nIt’s trivial to see that an associative memory matrix trained through this type of recurrent update rule will take the form \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] where \\(w_{l'} \\geq 0\\) is the weight assigned to token \\(l'\\) in the memory. I will refer to key-value associative memories with this structure as “weighted linear attention” modules.\nTo illustrate the connection between the recurrent update and the weighted linear attention form, consider unrolling the recurrence. Starting with \\[\nJ_l = \\omega_{f,l} J_{l-1} + \\omega_{i,l}\\, \\vec{v}_l\\, \\vec{k}_l^T,\n\\] recursively substituting \\(J_{l-1}\\) gives \\[\nJ_l = \\sum_{l'=1}^{l} \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\, \\vec{v}_{l'}\\, \\vec{k}_{l'}^T.\n\\] This shows that the associative memory \\(J_l\\) is a weighted sum of outer products of values and keys, where each token’s contribution is scaled by a weight \\(w_{l'} = \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\). These weights capture how much past tokens are retained in memory, clarifying how stateful, linear attention aggregates information over time.\nAlthough much of the practical interest in linear attention architectures is the ability to define them through recurrent update rules, it’s not clear that these recurrent update rules optimize their memory capacity. That is, there may be alternative ways to learn the weights that lead to better performing associative memories, at least in theory. I’ll explore this in a series of upcoming blog posts focused on a surprising connection between weighted linear attention modules and ecological systems.\nSpecifically, I will show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. In a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention. Follow along with the next post in this series Weighted Linear Attention is Lotka-Volterra Dynamics.\n\nThe following table from Yang et al (2024) highlights a number of models that use various types of linear attention modules and recurrent update rules:\n\n\n\n\n\n\nFigure 1: Types of Recurrent Updates for Linear Attention. (Source: S. Yang et al. (2024))"
  },
  {
    "objectID": "posts/weighted-linear-attention-as-species-invasion/index.html",
    "href": "posts/weighted-linear-attention-as-species-invasion/index.html",
    "title": "Weighted Linear Attention as Species Invasion",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. In previous posts, I showed that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. This post focused on the streaming context to show that online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention."
  },
  {
    "objectID": "posts/weighted-linear-attention-as-species-invasion/index.html#tldr",
    "href": "posts/weighted-linear-attention-as-species-invasion/index.html#tldr",
    "title": "Weighted Linear Attention as Species Invasion",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. In previous posts, I showed that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. This post focused on the streaming context to show that online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention."
  },
  {
    "objectID": "posts/weighted-linear-attention-as-species-invasion/index.html#review-of-previous-results",
    "href": "posts/weighted-linear-attention-as-species-invasion/index.html#review-of-previous-results",
    "title": "Weighted Linear Attention as Species Invasion",
    "section": "Review of Previous Results",
    "text": "Review of Previous Results\nIn the previous post Weighted Linear Attention is Lotka-Volterra Dynamics, I showed that weighted linear attention modules can be interpreted as ecological systems where tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. To briefly review those results, I focus on a particular form of weighted linear attention module determined by an associative memory matrix, \\[\nJ = \\sum_{l} w_{l} \\, \\vec{v}_{l} \\, \\vec{k}_{l}^T \\,,\n\\] where \\(l\\) denotes the token position, \\(\\vec{v}_l = W_{V} \\vec{x}_l \\in \\mathbb{R}^{d_v}\\) is a value vector, \\(\\vec{k}_l = W_{K} \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a key vector, \\(\\vec{x}_l \\in \\mathbb{R}^{n}\\) is a token embedding, and \\(w_l \\geq 0\\) is the weight of token \\(l\\) in memory. Recall from the memory is simply matrix multiplication, \\[\n\\tilde{v}_l = J \\, \\vec{q}_l\n\\] where \\(\\vec{q}_l = W_Q \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a query vector. The mean squared recall error for a batch of tokens \\(\\{ \\vec{x}_l \\}_{l=1}^L\\) is (up to a factor of \\(1/2\\)), \\[\nC(\\vec{w}) = \\frac{1}{2 L} \\sum_{l=1}^L || \\vec{v}_l - J \\, \\vec{q}_l ||^2 \\, .\n\\qquad(1)\\] To learn the weights, the cost function can be minimized using a simple variant of exponentiated gradient descent to satisfy the non-negativity constraint (Kivinen and Warmuth 1995). In the continuous-time limit, this update rule leads to the following differential equation \\[\n\\frac{d \\, w_l}{d \\, t} = - w_l \\frac{\\partial C}{\\partial w_l}\n\\] that describes the dynamics of the weights under the exponentiated gradient descent. After a bit of algebra, it’s possible to show that the following generalized Lokta-Volterra equation describes the dynamics of the weights, \\[\n\\frac{d \\, w_l}{d \\, t} =  w_l \\Big( s_l - \\sum_{l'} A_{l, l'} \\, w_{l'} \\Big) \\,,\n\\qquad(2)\\] where \\[\n\\begin{align}\ns_l &= \\vec{k}_l^T \\, \\Sigma_{qv} \\, \\vec{v}_l \\,,\n\\\\\nA_{l,l'} &= \\vec{v}_l^T \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T \\, \\Sigma_{qq} \\, \\vec{k}_{l}\n\\, ,\n\\end{align}\n\\] \\(s_l\\) is the intrinsic growth rate of token \\(l\\), \\(A_{l,l'}\\) is the interaction coefficient for tokens \\(l\\) and \\(l'\\), \\(\\Sigma_{qv}\\) is the uncentered query-value correlation matrix, and \\(\\Sigma_{qq}\\) is the uncentered query-query correlation matrix."
  },
  {
    "objectID": "posts/weighted-linear-attention-as-species-invasion/index.html#learning-algorithms",
    "href": "posts/weighted-linear-attention-as-species-invasion/index.html#learning-algorithms",
    "title": "Weighted Linear Attention as Species Invasion",
    "section": "Learning Algorithms",
    "text": "Learning Algorithms\nThe steps for learning the weights of the associative memory are shown as a code snippet below:\ndef lv_memory(w0, q, k, v, t_max, dt=0.01):\n    \"\"\"Compute memory matrix J from a full batch \n    of tokens using Lotka-Volterra dynamics\"\"\"\n    Sigma_vv, Sigma_qv, Sigma_qq = compute_correlations(q, v)\n    s, A = ecological_params(k, v, Sigma_qv, Sigma_qq)\n    w = integrate_lv(w0, s, A, t_max, dt)\n    J = torch.einsum('l,li,lj-&gt;ij', w, v, k)    \n    return J\nEssentially, learning in full batch mode amounts to calculating the correlation matrices, computing the growth rates and interaction coefficients, and then integrating the Lotka-Volterra equations. An example with randomly generated synthetic data is shown in Figure 1. The simulations show that there is wide variation in the growth rates and the interaction coefficients of the tokens, indicating that certain tokens are much more important for the memory than others. Applying exponentiated gradient descent with automatic differentiation through the squared reconstruction error results in identical to the dynamics loss curves compared to integrating the Lotka-Volterra equations, indicating the derivations are correct. As expected, the loss decreases monotonically as the weights converge to a fixed point.\n\n\n\n\n\n\nFigure 1: Synthetic example data. I randomly generated \\(Q\\), \\(K\\), and \\(V\\) matrices with covariance matrices (A) {vv}\\(, (B) \\Sigma_{qq}\\), and (C) {qv}$. The resulting growth rates and interaction coefficients are shown in panels D and E. Panel F shows loss curves for the Lotka-Volterra dynamics along with an implementation that directly minimizes the squared loss function through automatic differentiation and exponentiated gradient descent. As expected, the loss curves are identical.\n\n\n\nStoring memories in full-batch mode is interesting from an interpretability standpoint because it allows one to understand linearized attention modules through an ecological lens. In practice, however, one is typically more interested in storing memories sequentially as new tokens arrive. The naive approach to sequential learning is to simply solve the full-batch algorithm again each time a new token arrives. However, this approach is inefficient because (i) it’s possible to warm-start the integrator from the previous solution and (ii) it’s not even necessary to update the weights if the new token can’t invade the ecosystem. It is more efficient to use an “invade and adjust” algorithm that updates the covariance matrices upon the arrival of a new token, then checks if the new token is able to invade the ecosystem and only adjusts the weights if necessary. For the Lotka-Volterra system, a new token \\(l\\) cannot invade the ecosystem if \\[\ns_l - \\sum_{l'=1}^{l-1} A_{l,l'} w_{l'} \\leq 0 \\,,\n\\] in which case \\(w_l\\) should be set to zero. An implementation of an invade and adjust algorithm for sequential memory updating is shown in the following code snippet:\ndef invade_and_adjust(Sigma_vv, Sigma_qv, Sigma_qq, q, v, k, V, K, l, w_prev, t_max=1, dt=0.01):\n    \"\"\"Update memory with new token using online \n    Lotka-Volterra dynamics\"\"\"\n    # Update statistics\n    l = l + 1\n    z = (l - 1) / l\n    Sigma_vv = z * Sigma_vv + (v @ v.T) / l\n    Sigma_qv = z * Sigma_qv + (q @ v.T) / l\n    Sigma_qq = z * Sigma_qq + (q @ q.T) / l\n\n    # Check invasion\n    s_l = (k.T @ Sigma_qv @ v).item()\n    A_l = (v.T @ V.T) * (K @ Sigma_qq @ k).T \n    margin = s_l - (A_l @ w_prev).item()\n\n    # Can't invade\n    if margin &lt;= 0:\n        return Sigma_vv, Sigma_qv, Sigma_qq, V, K, w_prev, l\n\n    # Can invade\n    V_new = torch.cat([V, v[None]], dim=0)\n    K_new = torch.cat([K, k[None]], dim=0)\n    s, A = compute_ecological_params(K_new, V_new, Sigma_qv, Sigma_qq)\n\n    # Warm start\n    eps = torch.tensor([1e-3])\n    w_new = torch.cat([w_prev, eps])\n    w_new = integrate_lv(w_new, s, A, t_max, dt)\n\n    return Sigma_vv, Sigma_qv, Sigma_qq, V_new, K_new, w_new, l\nSimilar invade and adjust style algorithms have previously been proposed for training support vector machines (Mehta et al. 2019; Howell et al. 2020). These algorithms are exact in the sense that weights are always at a fixed point of the dynamical system, provided the system is allowed to reach a equilibrium after a successful invasion. This equilibrium point is not necessarily unique if it lies on the boundary, however. Instead, the weights of the tokens in the memory will depend on their order of arrival. The dependence of the order of the arrival of the species on the composition of the community is a well-known problem in ecology known as the historical contingency of community assembly (Fukami 2015).\nAlthough the invade and adjust algorithm is more efficient than solving the Lotka-Volterra from scratch with each new token, it requires computation of an \\(l \\times l\\) interaction matrix just as with full-batch training. Thus, exact algorithms for learning the weights in weighted linear attention are quadratic in the sequence length. One of the main goals for exploring different types of linearized attention modules is to develop algorithms that are linear in sequence length, therefore it’s necessary to introduce some approximations to develop algorithms that have the desired scaling properties.\nA simple approximation is to assume that the arrival of a new token \\(l\\) updates the memory matrix as \\[\nJ_l = \\omega_f \\, J_{l-1} + \\omega_i \\vec{v}_l \\vec{k}_l^T \\,,\n\\qquad(3)\\] where \\(\\omega_f \\geq 0\\) is a forget gate and \\(\\omega_i \\geq 0\\) is an input gate (Beck et al. 2025). This assumes that the incoming token \\(l\\) interacts with a fixed memory defined by \\(J_{l-1}\\) and the arrival of the new token does not adjust the weights of any previous tokens. Given that this is a type of greedy update rule, I refer to this as greedy invasion. With this assumption, the cost function becomes, \\[\n\\begin{align}\nC_l(\\omega_i)\n&= \\frac{1}{2} \\text{Tr}\\Big( \\Sigma_{vv}^l \\Big)\n- \\omega_f \\, s_J + \\frac{1}{2} \\omega_f^2 \\, A_{J,J}\n\\\\\n&\\quad - \\omega_i s_l + \\frac{1}{2} \\omega_i^2 A_{l,l}\n+ \\omega_i \\, \\omega_f \\, A_{J,l}\n\\end{align}\n\\] where \\[\n\\begin{align}\ns_{J} &=  \\text{Tr}\\Big(J_{l-1} \\, \\Sigma_{qv}^l \\Big)\n\\\\\ns_{l} &= \\vec{k}_l^T \\, \\Sigma_{qv}^l \\, \\vec{v}_l\n\\\\\nA_{J, J} &= \\text{Tr}\\Big(J_{l-1} \\, \\Sigma_{qq}^l \\, J_{l-1}^T \\Big)\n\\\\\nA_{l, l} &= \\vec{v}_l^T \\, \\vec{v}_l \\, \\vec{k}_l^T \\, \\Sigma_{qq}^l \\, \\vec{k}_l\n\\\\\nA_{J, l} &= \\vec{v}_l^T \\, J_{l-1} \\, \\Sigma_{qq}^l \\, \\vec{k}_l\n\\end{align}\n\\] are online estimates for the intrinsic growth rates and interactions. As in the previous section on full-batch learning, learning the gates with exponentiated gradient descent in this context leads to a system of two coupled ordinary differential equations \\[\n\\begin{align}\n\\frac{d \\, \\omega_f}{d \\, t} &= \\omega_f \\left( s_J - \\omega_i A_{J,l} - A_{J,J} \\omega_f \\right)\n\\\\\n\\frac{d \\, \\omega_i}{d \\, t} &= \\omega_i \\left( s_l - \\omega_f A_{J,l} - A_{l,l} \\omega_i \\right)\n\\end{align}\n\\] describing a Lotka-Volterra model with two species. Assuming that the interaction matrix is invertible, the unconstrained solution is \\[\n\\begin{align}\n\\omega_f^* &= \\frac{A_{l,l} \\, s_J - A_{J,l} \\, s_l}{A_{J,J} \\, A_{l,l} - A_{J,l}^2},\n\\\\\n\\omega_i^* &= \\frac{A_{J,J} \\, s_l - A_{J,l} \\, s_J}{A_{J,J} \\, A_{l,l} - A_{J,l}^2}\\,.\n\\end{align}\n\\] Taking into account the non-negativity constraints yields \\[\n\\begin{align}\n\\omega_f &=\n\\begin{cases}\n\\omega_f^*, & \\text{if } \\omega_i^* &gt; 0, \\\\[2mm]\n\\displaystyle \\frac{s_J}{A_{J,J}}, & \\text{if } \\omega_i^* \\le 0,\n\\end{cases}\\\\[2mm]\n\\omega_i &=\n\\begin{cases}\n\\omega_i^*, & \\text{if } \\omega_f^* &gt; 0,\\\\[2mm]\n\\displaystyle \\frac{s_l}{A_{l,l}}, & \\text{if } \\omega_f^* \\le 0,\n\\end{cases}\n\\end{align}\n\\] which is a closed-form solution for optimal gated linear attention. Alternatively, one could simplify the system even further by treating \\(\\omega_f\\) as a user specified hyperparameter, in which case \\[\n\\omega_i = \\max \\Big\\{0, \\frac{s_l - \\omega_f A_{J,l}}{A_{l,l}} \\Big\\} \\,,\n\\] is the corresponding optimal input gate. In either case, the memory matrix is updated using Equation 3. This greedy invasion update can be interpreted as adapting the carrying capacity of the new token in response to ecological pressure—i.e., competition or cooperation—with the current contents of memory. The algorithm is shown as a code snippet in the following code snippet:\ndef greedy_invasion_update(J_prev, v, k, Sigma_qv, Sigma_qq):\n    \"\"\"Lotka-Volterra-inspired gated update \n    to memory matrix\"\"\"\n    # Compute ecological parameters\n    s_J = torch.trace(J_prev @ Sigma_qv)\n    s_l = k @ Sigma_qv @ v\n    A_JJ = torch.trace(J_prev @ Sigma_qq @ J_prev.T)\n    A_ll = (v @ v) * (k @ Sigma_qq @ k)\n    A_Jl = v @ J_prev @ Sigma_qq @ k\n\n    # Compute unconstrained gates\n    denom = A_JJ * A_ll - A_Jl**2\n    omega_f = (A_ll * s_J - A_Jl * s_l) / denom\n    omega_i = (A_JJ * s_l - A_Jl * s_J) / denom\n\n    # Apply non-negativity constraints\n    if omega_i &lt;= 0:\n        omega_f = s_J / A_JJ\n        omega_i = 0.0\n    elif omega_f &lt;= 0:\n        omega_i = s_l / A_ll\n        omega_f = 0.0\n\n    # Gated memory update\n    J_new = omega_f * J_prev + omega_i * torch.outer(v, k)\n    return J_new\nFigure 2 compares the performance of the invade-and-adjust algorithm to greedy invasion, along with a baseline of linear attention with equally weighted memories. The invade-and-adjust algorithm has the lowest squared error for all token counts, which is expected because it provides a theoretical lower bound. Both the invade-and-adjust and greedy invasion algorithms significantly outperform the baseline of equally weighted linear attention, with greedy invasion performing slightly worse than invade-and-adjust even though the former is linear in sequence length whereas the latter is quadratic. The ecologically inspired updates improve over standard linear attention because they do not store all tokens equally and, in fact, do not even store all tokens as shown in the bottom panel of Figure 2.\n\n\n\n\n\n\nFigure 2: Comparison of memory update algorithms. (Top) The ecological methods (Invade-and-Adjust, Greedy Invasion) consistently achieve lower squared reconstruction errors (cost) than equally weighted linear attention by selectively updating memory based on invasion fitness. (Bottom) Cumulative number of tokens stored. Ecological updates adaptively reject redundant or non-informative tokens, leading to more compact and efficient memory representations.\n\n\n\nComparing these different variations of weighted linear attention raises some potentially interesting questions about the tradeoffs associated with these approaches. Invade-and-adjust provably minimizes the squared reconstruction error, but this comes at the cost of complexity that is the square of the sequence length. Greedy invasion, by contrast, is fully recurrent and linear in the sequence length but it does not have an obvious parallel implementation. Linear attention, of course, is both linear in the sequence length and easy to parallelize. The simulations presented here only explore a single realization of the dynamics of memory formation with some synthetic data and don’t constitute a rigorous characterization of these tradeoffs, but this mapping to ecological dynamics should provide a rich theoretical framework to explore these tradeoffs in future work."
  },
  {
    "objectID": "posts/weighted-linear-attention-as-species-invasion/index.html#conclusion",
    "href": "posts/weighted-linear-attention-as-species-invasion/index.html#conclusion",
    "title": "Weighted Linear Attention as Species Invasion",
    "section": "Conclusion",
    "text": "Conclusion\nIn this post, I’ve show that within a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. And, I used this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention. So far, this series of posts has focused on the theory linking attention mechanisms to ecological systems rather than the exploration of practical implications, but I speculate that this theoretical framework will be useful for developing ecology-inspired approaches to mechanistic interpretability, KV-cache compression, and the design of subquadratic attention modules.\nSo far, the theoretical framework I’ve been developing hinges on learning the weights in the weighted linear attention module in order to minimize the squared reconstruction error of the recalled values. In the next post, I’ll show that this framework is actually more general and similar dynamical models arise from a wide class of loss functions."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. Here, I show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#tldr",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#tldr",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. Here, I show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#theory",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#theory",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "Theory",
    "text": "Theory\nAlthough ecology and evolutionary biology have inspired a variety of computational algorithms particularly in the area of non-convex optimization (Storn and Price 1997; Binitha, Sathya, et al. 2012), there is little exploration of the relationship between ecological systems and neural networks. In a classical paper on theoretical ecology, Robert MacArthur showed that the Lotka-Volterra equations of competitive ecosystems minimize a quadratic Lyapunov function (Arthur 1969; MacArthur 1970), establishing an interesting connection between ecological systems and non-negative least squares regression problems. Initially, this insight was primarily used to better understand the behavior of ecological communities, but recent work has taken the other direction and shown that ecological models can be applied to machine learning problems like training support vector machines (Mehta et al. 2019; Howell et al. 2020).\nIn this post, I show that there is an exact correspondence between associative memories in the form of weighted linear attention and ecological systems described by Lotka-Volterra dynamics (Wangersky 1978; Schuster and Sigmund 1983; Bomze 1983, 1995; Cui, Marsland III, and Mehta 2024). This mapping opens up new avenues for interpreting attention modules and other types of associative memories in terms of well-established concepts from theoretical ecology and may also inspire new ways to interpret ecological dynamics using concepts from machine learning. Specifically, each token in a sequence corresponds to a species in an ecosystem. The weight of the token in the memory corresponds to its species abundance. The arrival of a new token in a data stream is equivalent to the invasion of the ecosystem by a new species. Tokens engage in competitive or mutualistic interactions determined by the statistics of their key, value, and query embeddings. The schematic in Figure 1 illustrates the high-level concepts linking ecology and attention.\n\n\n\n\n\n\nFigure 1: Mapping linear attention to ecology. Schematic comparing (A) species interacting in an ecological community to (B) tokens interacting within a context window.\n\n\n\nLinearized attention modules are an important area of research in machine learning as a potential alternative to softmax attention heads in transformers including modules such as linear attention, DeltaNet, xLSTMs, and state-space models that scale linearly with sequence length . Here, I focus on a particular form of weighted linear attention module determined by an associative memory matrix, \\[\nJ = \\sum_{l} w_{l} \\, \\vec{v}_{l} \\, \\vec{k}_{l}^T \\,,\n\\] where \\(l\\) denotes the token position, \\(\\vec{v}_l = W_{V} \\vec{x}_l \\in \\mathbb{R}^{d_v}\\) is a value vector, \\(\\vec{k}_l = W_{K} \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a key vector, \\(\\vec{x}_l \\in \\mathbb{R}^{n}\\) is a token embedding, and \\(w_l \\geq 0\\) is the weight of token \\(l\\) in memory. Recall from the memory is simply matrix multiplication, \\[\n\\tilde{v}_l = J \\, \\vec{q}_l\n\\] where \\(\\vec{q}_l = W_Q \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a query vector. The task is how to specify the weights of the patterns in the associative memory in order to minimize the error in the recalled value vector. I’ve provided some background on weighted linear attention modules in a previous post.\nThe mean squared recall error for a batch of tokens \\(\\{ \\vec{x}_l \\}_{l=1}^L\\) is (up to a factor of \\(1/2\\)), \\[\nC(\\vec{w}) = \\frac{1}{2 L} \\sum_{l=1}^L || \\vec{v}_l - J \\, \\vec{q}_l ||^2 \\, .\n\\qquad(1)\\] Thus, a sensible strategy is to choose the weights to minimize the squared error subject to the non-negativity constraint. This is closely related to a recently introduced framework called ``test-time regression’’ aiming to unify different methods for associative recall (Wang, Shi, and Fox 2025; Kohonen 1972; Hinton and Anderson 2014). It turns out that the results that follow are largely applicable for a wide class of loss functions, which I’ll demonstrate in a future blog post. To learn the weights, the cost function can be minimized using a simple variant of exponentiated gradient descent to satisfy the non-negativity constraint (Kivinen and Warmuth 1995). The update rule for exponentiated gradient descent with non-negative weights is \\[\nw_l' = w_l e^{-\\eta \\frac{\\partial C}{\\partial w_l}} \\, .\n\\] In the continuous-time limit (i.e., very small \\(\\eta\\)), this update rule leads to the following differential equation \\[\n\\frac{d \\, w_l}{d \\, t} = - w_l \\frac{\\partial C}{\\partial w_l}\n\\] that describes the dynamics of the weights under the exponentiated gradient descent.\nAfter a bit of algebra, it’s possible to compute the averages over the batch in order to derive the squared recall error \\[\nC(\\vec{w})\n= \\frac{1}{2} \\text{Tr}\\Big( \\Sigma_{vv} \\Big)\n- \\text{Tr}\\Big(J \\, \\Sigma_{qv} \\Big)\n+ \\frac{1}{2} \\text{Tr}\\Big(J \\, \\Sigma_{qq} \\, J^T \\Big)\n\\] where \\[\n\\begin{align}\n\\Sigma_{vv} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{v}_l \\, \\vec{v}_l^T \\\\\n\\Sigma_{qv} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{q}_l \\, \\vec{v}_l^T \\\\\n\\Sigma_{qq} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{q}_l \\, \\vec{q}_l^T\n\\end{align}\n\\] are observed correlation matrices. The derivative of the cost function with respect to the weights is \\[\n\\frac{\\partial C}{\\partial w_l}\n= - s_l + \\sum_{l'} A_{l, l'} \\, w_{l'} \\,.\n\\] where \\[\n\\begin{align}\ns_l &= \\vec{k}_l^T \\, \\Sigma_{qv} \\, \\vec{v}_l \\,,\n\\\\\nA_{l,l'} &= \\vec{v}_l^T \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T \\, \\Sigma_{qq} \\, \\vec{k}_{l}\n\\, .\n\\end{align}\n\\] In analogy with the ecological literature, I call \\(s_l\\) the “intrinsic growth rate” of token \\(l\\) and \\(A_{l,l'}\\) the “interaction coefficient” between tokens \\(l\\) and \\(l'\\). The following code illustrates how to compute these quantities:\ndef compute_correlations(q, v):\n    L = q.shape[0]\n    Sigma_vv = v.T @ v / L\n    Sigma_qv = q.T @ v / L\n    Sigma_qq = q.T @ q / L\n    return Sigma_vv, Sigma_qv, Sigma_qq\n\ndef ecological_params(K, V, Sigma_qv, Sigma_qq):\n    s = (K @ Sigma_qv * V).sum(dim=1)\n    A = (V @ V.T) * (K @ Sigma_qq @ K.T)\n    return s, A\nPlugging in these results yields the following differential equation describing the dynamics of the weights, \\[\n\\frac{d \\, w_l}{d \\, t} =  w_l \\Big( s_l - \\sum_{l'} A_{l, l'} \\, w_{l'} \\Big) \\,,\n\\qquad(2)\\] which is exactly the generalized Lotka-Volterra equation. A simple algorithm for integrating the Lotka-Volterra equations is shown in the following code snippet:\ndef integrate_lv(w, s, A, t_max, dt=0.01):\n    t = 0\n    while t &lt; t_max:\n        w = w + dt * w * (s - A @ w)\n        w = w.clamp(min=0)\n        t += dt\n    return w\nAlthough relatively straightforward, as far as I know the derivation of Equation 2 as a way to minimize the squared recall error for weighted linear attention is a new result.\nThis construction allows one to directly interpret the terms in Equation 2 as ecologically inspired quantities. For example, a token is a species. The weight of a token is the abundance of the species. The intrinsic growth rate of species \\(l\\) is \\(s_l\\), which defines how quickly the weight of the corresponding token increases at the start of learning. The total weight of a token is generally limited by its intrinsic growth rate \\(s_l\\) and self-interaction \\(A_{l,l'}\\) via a quantity known as its carrying capacity \\[\n\\kappa_l\n= \\frac{s_l}{A_{l,l}}\n= \\frac{\\vec{k}_l^T \\, \\Sigma_{qv} \\, \\vec{v}_l}{\\vec{v}_l^T \\, \\vec{v}_{l} \\, \\vec{k}_{l}^T \\, \\Sigma_{qq} \\, \\vec{k}_{l}} \\,,\n\\] which determines the weight of a token in the absence of interactions with the other tokens.\nOf course, tokens do interact with each other. Two tokens compete if \\(A_{l,l'} = A_{l', l} &gt; 0\\) and they cooperate if \\(A_{l,l'} = A_{l', l} &lt; 0\\). There are no predator-prey style interactions (i.e., \\(A_{l,l'} = -A_{l', l}\\)) in this model because the interaction matrix is symmetric. Since the interaction matrix is symmetric, the species abundances will converge to a fixed point without any cycles. The interior fixed point is unique if \\(A^{-1} \\, \\vec{s}\\) is strictly positive, otherwise there can be multiple fixed points on the boundary in which one, or multiple, species are extinct.\nFigure 2 shows how the interactions between tokens in the context window are determined by the alignment of their value vectors and the correlation of their attention scores.\n\n\n\n\n\n\nFigure 2: Interpreting interactions. Interactions between tokens are related to the correlation of their attention scores and the alignment of their value vectors.\n\n\n\nTo illustrate this, define the attention score \\(\\theta_{l,l'} = \\vec{k}_l^T \\, \\vec{q}_{l'}\\) and the alignment score \\(\\phi_{l,l'} = \\vec{v}_l^T \\, \\vec{v}_{l'}\\) between tokens \\(l\\) and \\(l'\\). Although the alignment scores are symmetric, the attention scores are not; token \\(l\\) may attend to token \\(l'\\) differently from the way token \\(l'\\) attends to token \\(l\\). By substituting the formula for the correlation matrix, one can derive an equation for the intrinsic growth rate of token \\(l\\) as \\[\n\\begin{align}\ns_l\n&= \\frac{1}{L} \\sum_{l'} \\theta_{l,l'} \\, \\phi_{l,l'} \\,,\n\\\\\n&\\sim \\text{Covariance}\\left(\\text{attention}, \\text{alignment}\\right)\n\\,.\n\\end{align}\n\\] Thus, the intrinsic growth rate of a token is the (uncentered) covariance between its attention scores and alignment scores. Token \\(l\\) will have a high growth rate if its value is aligned with the values of the other tokens that attend to it. Similarly, the self-interaction is \\[\n\\begin{align}\nA_{l,l}\n&= \\phi_{l,l} \\, \\left( \\frac{1}{L} \\sum_{l'} \\theta_{l,l'}^2 \\right) \\,,\n\\\\\n&\\sim \\text{Norm}^2\\left(\\text{value}\\right) \\cdot \\text{Variance}\\left(\\text{attention}\\right)\n\\, .\n\\end{align}\n\\] Thus, the self-interaction coefficient for a token \\(l\\) is the squared norm of its value vector multiplied by the (uncentered) variance of its attention scores. Since the carrying capacity is the ratio of the growth rate to the self-interaction, it looks like the covariance between the attention and alignment scores divided by the variance of the attention scores and the norm of the value vector. The interaction between different tokens \\(l\\) and \\(l'\\) has a similar interpretation \\[\n\\begin{align}\nA_{l,l'}\n&= \\phi_{l,l'} \\left( \\frac{1}{L} \\sum_{l''} \\theta_{l,l''} \\, \\theta_{l', l''} \\right)\n\\,, \\\\\n&\\sim \\text{Alignment} \\cdot \\text{Covariance}\\left( \\text{attention}, \\text{attention} \\right)\n\\,.\n\\end{align}\n\\] Thus, two tokens are in competition if their values are aligned and they are attended to by similar tokens. Such tokens provide similar information, so there is no need to store both of them if the memory has limited capacity. Interestingly, tokens can also cooperate with each other. This happens if their values are aligned but they have anti-correlated attention scores, or if they have opposite values with positively correlated attention maps."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#conclusion",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#conclusion",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "Conclusion",
    "text": "Conclusion\nAttention mechanisms have become the cornerstone of modern sequence models. However, because the standard formulation of attention scales with the length of the context window squared, there has been a lot of recent interest in alternative approaches with linear scaling. Most of these alternative architectures involve weighted linear attention modules of some form. So far, these architectures based on key-value associative memories have generally fallen short of their transformer counterparts, raising an important question of `why?’.\nHere, I have provided a new lens through which to view attention mechanisms–that of ecosystems. Specifically, I’ve shown that weighted associative memories based on query-key-value recall mechanisms correspond to ecological communities in which the tokens are species and the weights in the memory are their species abundances. In fact, this is more than a colorful metaphor, and the dynamics of exponentiated gradient descent on a squared reconstruction loss are exactly described by Lotka-Volterra dynamics.\nThis theory highlights some simple tools for understanding the inner workings of attention mechanisms. For example, the covariance between a token’s attention scores and its alignment scores determines its intrinsic growth rate. However, the intrinsic growth rate is, by itself, not enough to specify the weight of a token in the memory because tokens also interact with each other. In fact, the alignment of two tokens value vectors and the covariance of their attention scores determines if the tokens compete with each for space in the memory, or if they actually cooperate and reinforce each other’s weights.\nIn the following post, Weighted Linear Attention as Species Invasion, I will show that within a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. And, I’ll use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nTechnology is often developed out in the open, through open source projects that allow one to follow along with the development process in addition to consuming the final product. But scientific research is usually done behind closed doors, for reasons I don’t entirely understand. The thinking process that leads to a paper isn’t shared, except among a few collaborators. This blog is my attempt at truly open scientific research.\nThe blog is called “Local Minimum” because many of the ideas I share here will be early and immature. It’s a nod to the way can systems can get stuck in a local minimum of an energy landscape.\n\n\nAbout the author\nCharles Fisher is typically interested in research at the intersection of physics, machine learning, and biology. He has a PhD in biophysics from Harvard University, a BS in biophysics from the University of Michigan, and did postdoctoral research in biophysics at Boston University and Ecole Normale Superieure in Paris. Following his time in academia, he moved to industry where he worked at Pfizer and Leap Motion before starting Unlearn.AI. He was CEO of Unlearn for nearly 8 years before stepping back to a board role so that he could get back to his roots and dive into research again. Hence, this blog.\n\n\nAcknowledgments\nI’m often discussing my research with Pankaj Mehta and his group at Boston University, Austin Huang, Anton Loukianov, and others."
  }
]