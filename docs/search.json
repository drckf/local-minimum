[
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "license.html#creative-commons-attribution-4.0-international-license",
    "href": "license.html#creative-commons-attribution-4.0-international-license",
    "title": "License",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License.\n\n\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material for any purpose, even commercially\n\n\n\n\n\nAttribution — You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.\n\n\n\n\nYou do not have to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation.\nNo warranties are given. The license may not give you all of the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.\n\n\n\nWhen citing posts from this blog, please use the following format:\nFisher, C. (Year). Title of post. Local Minimum. URL\nFor example:\nFisher, C. (2025). What is Weighted Linear Attention? Local Minimum. https://local-minimum.com/posts/weighted-linear-attention/\nIf you’re using BibTeX, you can use this format:\n@misc{fisher2025weighted,\n  author = {Fisher, Charles},\n  title = {What is Weighted Linear Attention?},\n  year = {2025},\n  howpublished = {Local Minimum},\n  url = {https://local-minimum.com/posts/weighted-linear-attention/}\n}\nPlease ensure you include the URL to the specific post you’re citing."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "index.html#welcome-to-the-local-minimum",
    "href": "index.html#welcome-to-the-local-minimum",
    "title": "Local Minimum",
    "section": "",
    "text": "This is a technical blog with long form posts about my research before it is ready for publication in traditional journal articles. Most of this research will be at the intersection of statistical physics, machine learning, and biology. Some posts will be reviews, but most will be involve novel research and will include both theoretical and experimental analyses, and sometimes just discussion of algorithms or code. For more information about this blog and its author, check out the About page."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html",
    "href": "posts/weighted-linear-attention/index.html",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#tldr",
    "href": "posts/weighted-linear-attention/index.html#tldr",
    "title": "What is Weighted Linear Attention?",
    "section": "",
    "text": "Weighted linear attention is a type of key-value associative memory defined by the matrix \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] and an associative recall formula \\(\\tilde{v}_l = J \\vec{q}_l\\) that retrieves a value correpsonding to a query vector. These modules are interesting components for neural sequence models because they can be viewed as alternatives to softmax attention that may be more efficient for long context windows."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#softmax-attention",
    "href": "posts/weighted-linear-attention/index.html#softmax-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Softmax Attention",
    "text": "Softmax Attention\nThe softmax attention module was a key innovation in the development of transformer models that are commonly used for natural language processing and other applications in machine learning (Vaswani et al. 2017). At a high level, attention leverages the idea that the meaning of a word in a document could change depending on the other words in the document. Therefore, in order to determine the meaning of a given word (or a component of a word called a token) we need to compare it to the rest of the words in the document.\nLet \\(\\vec{x}_l \\in \\mathbb{R}^n\\) for \\(l = 1, \\ldots, L\\) be a sequence of tokens, \\(W_Q\\) and \\(W_K\\) be \\(d_k \\times n\\) matrices, and \\(W_V\\) be a \\(d_v \\times n\\) matrix. The queries, keys, and values are given by \\[\n\\begin{align}\n\\vec{q}_l &= W_Q \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{k}_l &= W_K \\, \\vec{x}_l \\in \\mathbb{R}^{d_k} \\\\\n\\vec{v}_l &= W_V \\, \\vec{x}_l \\in \\mathbb{R}^{d_v}\n\\end{align}\n\\] The softmax attention module defines an \\(L \\times L\\) matrix \\(\\mathcal{A}\\) with elements \\[\n\\mathcal{A}_{l,l'} = \\frac{e^{ d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l'} }}{ \\sum_{l''} e^{d_k^{-1/2} \\vec{q}_l^T \\vec{k}_{l''} }}\n\\] that describes how query \\(l\\) attends to key \\(l'\\). Thus, the value retrived for query \\(l\\) is \\[\n\\tilde{v}_l = \\sum_{l'} \\mathcal{A}_{l,l'} \\vec{v}_{l'}\n\\] Of course, there are many variations of this general module that have been introduced in the last few years, but I’m not going to get into those because the original formulation has the necessary elements for this introduction.\nSoftmax attention is relatively easy to implement in parallel because it is stateless. This means that it efficiently utilizes the capabilities of modern GPUs, but it also requires a large amount of memory and computational resources because it involves computing an \\(L \\times L\\) matrix. Thus, softmax attention uses resources that are quadratic in the length of the context window. Although one can use a variety of tricks to mitigate this problem, it nevertheless represents a fundamental limitation to the length of sequences that transformers are able to process."
  },
  {
    "objectID": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "href": "posts/weighted-linear-attention/index.html#weighted-linear-attention",
    "title": "What is Weighted Linear Attention?",
    "section": "Weighted Linear Attention",
    "text": "Weighted Linear Attention\nIn the last few years, there has been renewed interest in stateful architectures for modeling sequential data such as language or time series (Wang, Shi, and Fox 2025; Aksenov et al. 2024; Schlag, Irie, and Schmidhuber 2021; Songlin Yang et al. 2025, 2023; Katharopoulos et al. 2020; H. Peng et al. 2021; Beck et al. 2025; Qin et al. 2022, 2024; Kasai et al. 2021; Zhang et al. 2024; Chen et al. 2024; Yutao Sun et al. 2023; Orvieto et al. 2023; Katsch 2023; De et al. 2024; B. Peng et al. 2024; Gu and Dao 2023; Dao and Gu 2024; Liu et al. 2024; Yu Sun et al. 2024; Songlin Yang, Kautz, and Hatamizadeh 2024; Behrouz, Zhong, and Mirrokni 2024). Stateful models are a more memory and compute efficient alternative to stateless models like transformers because they scale linearly with the length of the sequence in contrast to the quadratic scaling of transformers. In principle, stateful language models could have much cheaper inference costs and would be able to operate over essentially infinite context windows.\nModern stateful sequence models typically use a key-value associative memory as an alternative to the softmax attention heads in transformers. A key-value associative memory is a \\(d_v \\times d_k\\) matrix \\(J_l\\) that is typically, but not necessarily, updated through a recurrent relation such as \\[\n\\begin{equation}\nJ_l = \\omega_{f,l} \\, J_{l-1} + \\omega_{i,l} \\, \\vec{v}_l \\, \\vec{k}_l^T\n\\end{equation}\n\\] where \\(\\omega_{f,l} \\geq 0\\) is a forget gate and \\(\\omega_{i,l} \\geq 0\\) is an input gate. The input and forget gates could be constants, functions of the current state, or could even be functions of all previous states in the sequence. A value is retrieved from the memory by multiplication with a query vector \\(\\tilde{v}_l = J \\, \\vec{q}_l\\), where I am denoting the retrieved value with a tilde to emphasize that \\(\\tilde{v}_i\\) will generally not equal \\(\\vec{v}_i\\) unless the memory has perfect recall. See Figure 1 below for examples of different linear attention mechanisms.\nIt’s trivial to see that an associative memory matrix trained through this type of recurrent update rule will take the form \\[\n\\begin{equation}\nJ = \\sum_{l'} w_{l'} \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T\n\\end{equation}\n\\] where \\(w_{l'} \\geq 0\\) is the weight assigned to token \\(l'\\) in the memory. I will refer to key-value associative memories with this structure as “weighted linear attention” modules.\nTo illustrate the connection between the recurrent update and the weighted linear attention form, consider unrolling the recurrence. Starting with \\[\nJ_l = \\omega_{f,l} J_{l-1} + \\omega_{i,l}\\, \\vec{v}_l\\, \\vec{k}_l^T,\n\\] recursively substituting \\(J_{l-1}\\) gives \\[\nJ_l = \\sum_{l'=1}^{l} \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\, \\vec{v}_{l'}\\, \\vec{k}_{l'}^T.\n\\] This shows that the associative memory \\(J_l\\) is a weighted sum of outer products of values and keys, where each token’s contribution is scaled by a weight \\(w_{l'} = \\left( \\prod_{l''=l'+1}^{l} \\omega_{f,l''} \\right) \\omega_{i,l'}\\). These weights capture how much past tokens are retained in memory, clarifying how stateful, linear attention aggregates information over time.\nAlthough much of the practical interest in linear attention architectures is the ability to define them through recurrent update rules, it’s not clear that these recurrent update rules optimize their memory capacity. That is, there may be alternative ways to learn the weights that lead to better performing associative memories, at least in theory. I’ll explore this in a series of upcoming blog posts focused on a surprising connection between weighted linear attention modules and ecological systems.\nSpecifically, I will show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token. In a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention. Follow along with the next post in this series Weighted Linear Attention is Lotka-Volterra Dynamics.\n\nThe following table from Yang et al (2024) highlights a number of models that use various types of linear attention modules and recurrent update rules:\n\n\n\n\n\n\nFigure 1: Types of Recurrent Updates for Linear Attention. (Source: S. Yang et al. (2024))\n\n\n\n\n\n© 2025 Charles Fisher. This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. Here, I show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#tldr",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#tldr",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "",
    "text": "Weighted linear attention modules are key-value associative memories with potential uses in neural sequence models used for tasks such as language modeling. Here, I show that weighted linear attention can be interpreted as an evolving ecological system in which tokens are species and their weights are species abundances. The weights evolve under Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters such as the carrying capacity and interaction coefficients of each token."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#theory",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#theory",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "Theory",
    "text": "Theory\nAlthough ecology and evolutionary biology have inspired a variety of computational algorithms particularly in the area of non-convex optimization (Storn and Price 1997; Binitha, Sathya, et al. 2012), there is little exploration of the relationship between ecological systems and neural networks. In a classical paper on theoretical ecology, Robert MacArthur showed that the Lotka-Volterra equations of competitive ecosystems minimize a quadratic Lyapunov function (Arthur 1969; MacArthur 1970), establishing an interesting connection between ecological systems and non-negative least squares regression problems. Initially, this insight was primarily used to better understand the behavior of ecological communities, but recent work has taken the other direction and shown that ecological models can be applied to machine learning problems like training support vector machines (Mehta et al. 2019; Howell et al. 2020).\nIn this post, I show that there is an exact correspondence between associative memories in the form of weighted linear attention and ecological systems described by Lotka-Volterra dynamics (Wangersky 1978; Schuster and Sigmund 1983; Bomze 1983, 1995; Cui, Marsland III, and Mehta 2024). This mapping opens up new avenues for interpreting attention modules and other types of associative memories in terms of well-established concepts from theoretical ecology and may also inspire new ways to interpret ecological dynamics using concepts from machine learning. Specifically, each token in a sequence corresponds to a species in an ecosystem. The weight of the token in the memory corresponds to its species abundance. The arrival of a new token in a data stream is equivalent to the invasion of the ecosystem by a new species. Tokens engage in competitive or mutualistic interactions determined by the statistics of their key, value, and query embeddings. The schematic in Figure 1 illustrates the high-level concepts linking ecology and attention.\n\n\n\n\n\n\nFigure 1: Mapping linear attention to ecology. Schematic comparing (A) species interacting in an ecological community to (B) tokens interacting within a context window.\n\n\n\nLinearized attention modules are an important area of research in machine learning as a potential alternative to softmax attention heads in transformers including modules such as linear attention, DeltaNet, xLSTMs, and state-space models that scale linearly with sequence length . Here, I focus on a particular form of weighted linear attention module determined by an associative memory matrix, \\[\nJ = \\sum_{l} w_{l} \\, \\vec{v}_{l} \\, \\vec{k}_{l}^T \\,,\n\\] where \\(l\\) denotes the token position, \\(\\vec{v}_l = W_{V} \\vec{x}_l \\in \\mathbb{R}^{d_v}\\) is a value vector, \\(\\vec{k}_l = W_{K} \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a key vector, \\(\\vec{x}_l \\in \\mathbb{R}^{n}\\) is a token embedding, and \\(w_l \\geq 0\\) is the weight of token \\(l\\) in memory. Recall from the memory is simply matrix multiplication, \\[\n\\tilde{v}_l = J \\, \\vec{q}_l\n\\] where \\(\\vec{q}_l = W_Q \\vec{x}_l \\in \\mathbb{R}^{d_k}\\) is a query vector. The task is how to specify the weights of the patterns in the associative memory in order to minimize the error in the recalled value vector. I’ve provided some background on weighted linear attention modules in a previous post.\nThe mean squared recall error for a batch of tokens \\(\\{ \\vec{x}_l \\}_{l=1}^L\\) is (up to a factor of \\(1/2\\)), \\[\nC(\\vec{w}) = \\frac{1}{2 L} \\sum_{l=1}^L || \\vec{v}_l - J \\, \\vec{q}_l ||^2 \\, .\n\\qquad(1)\\] Thus, a sensible strategy is to choose the weights to minimize the squared error subject to the non-negativity constraint. This is closely related to a recently introduced framework called ``test-time regression’’ aiming to unify different methods for associative recall (Wang, Shi, and Fox 2025; Kohonen 1972; Hinton and Anderson 2014). It turns out that the results that follow are largely applicable for a wide class of loss functions, which I’ll demonstrate in a future blog post. To learn the weights, the cost function can be minimized using a simple variant of exponentiated gradient descent to satisfy the non-negativity constraint (Kivinen and Warmuth 1995). The update rule for exponentiated gradient descent with non-negative weights is \\[\nw_l' = w_l e^{-\\eta \\frac{\\partial C}{\\partial w_l}} \\, .\n\\] In the continuous-time limit (i.e., very small \\(\\eta\\)), this update rule leads to the following differential equation \\[\n\\frac{d \\, w_l}{d \\, t} = - w_l \\frac{\\partial C}{\\partial w_l}\n\\] that describes the dynamics of the weights under the exponentiated gradient descent.\nAfter a bit of algebra, it’s possible to compute the averages over the batch in order to derive the squared recall error \\[\nC(\\vec{w})\n= \\frac{1}{2} \\text{Tr}\\Big( \\Sigma_{vv} \\Big)\n- \\text{Tr}\\Big(J \\, \\Sigma_{qv} \\Big)\n+ \\frac{1}{2} \\text{Tr}\\Big(J \\, \\Sigma_{qq} \\, J^T \\Big)\n\\] where \\[\n\\begin{align}\n\\Sigma_{vv} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{v}_l \\, \\vec{v}_l^T \\\\\n\\Sigma_{qv} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{q}_l \\, \\vec{v}_l^T \\\\\n\\Sigma_{qq} &= \\frac{1}{L} \\sum_{l=1}^L \\vec{q}_l \\, \\vec{q}_l^T\n\\end{align}\n\\] are observed correlation matrices. The derivative of the cost function with respect to the weights is \\[\n\\frac{\\partial C}{\\partial w_l}\n= - s_l + \\sum_{l'} A_{l, l'} \\, w_{l'} \\,.\n\\] where \\[\n\\begin{align}\ns_l &= \\vec{k}_l^T \\, \\Sigma_{qv} \\, \\vec{v}_l \\,,\n\\\\\nA_{l,l'} &= \\vec{v}_l^T \\, \\vec{v}_{l'} \\, \\vec{k}_{l'}^T \\, \\Sigma_{qq} \\, \\vec{k}_{l}\n\\, .\n\\end{align}\n\\] In analogy with the ecological literature, I call \\(s_l\\) the “intrinsic growth rate” of token \\(l\\) and \\(A_{l,l'}\\) the “interaction coefficient” between tokens \\(l\\) and \\(l'\\). The following code illustrates how to compute these quantities:\ndef compute_correlations(q, v):\n    L = q.shape[0]\n    Sigma_vv = v.T @ v / L\n    Sigma_qv = q.T @ v / L\n    Sigma_qq = q.T @ q / L\n    return Sigma_vv, Sigma_qv, Sigma_qq\n\ndef ecological_params(K, V, Sigma_qv, Sigma_qq):\n    s = (K @ Sigma_qv * V).sum(dim=1)\n    A = (V @ V.T) * (K @ Sigma_qq @ K.T)\n    return s, A\nPlugging in these results yields the following differential equation describing the dynamics of the weights, \\[\n\\frac{d \\, w_l}{d \\, t} =  w_l \\Big( s_l - \\sum_{l'} A_{l, l'} \\, w_{l'} \\Big) \\,,\n\\qquad(2)\\] which is exactly the generalized Lotka-Volterra equation. A simple algorithm for integrating the Lotka-Volterra equations is shown in the following code snippet:\ndef integrate_lv(w, s, A, t_max, dt=0.01):\n    t = 0\n    while t &lt; t_max:\n        w = w + dt * w * (s - A @ w)\n        w = w.clamp(min=0)\n        t += dt\n    return w\nAlthough relatively straightforward, as far as I know the derivation of Equation 2 as a way to minimize the squared recall error for weighted linear attention is a new result.\nThis construction allows one to directly interpret the terms in Equation 2 as ecologically inspired quantities. For example, a token is a species. The weight of a token is the abundance of the species. The intrinsic growth rate of species \\(l\\) is \\(s_l\\), which defines how quickly the weight of the corresponding token increases at the start of learning. The total weight of a token is generally limited by its intrinsic growth rate \\(s_l\\) and self-interaction \\(A_{l,l'}\\) via a quantity known as its carrying capacity \\[\n\\kappa_l\n= \\frac{s_l}{A_{l,l}}\n= \\frac{\\vec{k}_l^T \\, \\Sigma_{qv} \\, bvec{v}_l}{\\vec{v}_l^T \\, \\vec{v}_{l} \\, \\vec{k}_{l}^T \\, \\Sigma_{qq} \\, \\vec{k}_{l}} \\,,\n\\] which determines the weight of a token in the absence of interactions with the other tokens.\nOf course, tokens do interact with each other. Two tokens compete if \\(A_{l,l'} = A_{l', l} &gt; 0\\) and they cooperate if \\(A_{l,l'} = A_{l', l} &lt; 0\\). There are no predator-prey style interactions (i.e., \\(A_{l,l'} = -A_{l', l}\\)) in this model because the interaction matrix is symmetric. Since the interaction matrix is symmetric, the species abundances will converge to a fixed point without any cycles. The interior fixed point is unique if \\(A^{-1} \\, \\vec{s}\\) is strictly positive, otherwise there can be multiple fixed points on the boundary in which one, or multiple, species are extinct.\nFigure 2 shows how the interactions between tokens in the context window are determined by the alignment of their value vectors and the correlation of their attention scores.\n\n\n\n\n\n\nFigure 2: Interpreting interactions. Interactions between tokens are related to the correlation of their attention scores and the alignment of their value vectors.\n\n\n\nTo illustrate this, define the attention score \\(\\theta_{l,l'} = \\vec{k}_l^T \\, \\vec{q}_{l'}\\) and the alignment score \\(\\phi_{l,l'} = \\vec{v}_l^T \\, \\vec{v}_{l'}\\) between tokens \\(l\\) and \\(l'\\). Although the alignment scores are symmetric, the attention scores are not; token \\(l\\) may attend to token \\(l'\\) differently from the way token \\(l'\\) attends to token \\(l\\). By substituting the formula for the correlation matrix, one can derive an equation for the intrinsic growth rate of token \\(l\\) as \\[\n\\begin{align}\ns_l\n&= \\frac{1}{L} \\sum_{l'} \\theta_{l,l'} \\, \\phi_{l,l'} \\,,\n\\\\\n&\\sim \\text{Covariance}\\left(\\text{attention}, \\text{alignment}\\right)\n\\,.\n\\end{align}\n\\] Thus, the intrinsic growth rate of a token is the (uncentered) covariance between its attention scores and alignment scores. Token \\(l\\) will have a high growth rate if its value is aligned with the values of the other tokens that attend to it. Similarly, the self-interaction is \\[\n\\begin{align}\nA_{l,l}\n&= \\phi_{l,l} \\, \\left( \\frac{1}{L} \\sum_{l'} \\theta_{l,l'}^2 \\right) \\,,\n\\\\\n&\\sim \\text{Norm}^2\\left(\\text{value}\\right) \\cdot \\text{Variance}\\left(\\text{attention}\\right)\n\\, .\n\\end{align}\n\\] Thus, the self-interaction coefficient for a token \\(l\\) is the squared norm of its value vector multiplied by the (uncentered) variance of its attention scores. Since the carrying capacity is the ratio of the growth rate to the self-interaction, it looks like the covariance between the attention and alignment scores divided by the variance of the attention scores and the norm of the value vector. The interaction between different tokens \\(l\\) and \\(l'\\) has a similar interpretation \\[\n\\begin{align}\nA_{l,l'}\n&= \\phi_{l,l'} \\left( \\frac{1}{L} \\sum_{l''} \\theta_{l,l''} \\, \\theta_{l', l''} \\right)\n\\,, \\\\\n&\\sim \\text{Alignment} \\cdot \\text{Covariance}\\left( \\text{attention}, \\text{attention} \\right)\n\\,.\n\\end{align}\n\\] Thus, two tokens are in competition if their values are aligned and they are attended to by similar tokens. Such tokens provide similar information, so there is no need to store both of them if the memory has limited capacity. Interestingly, tokens can also cooperate with each other. This happens if their values are aligned but they have anti-correlated attention scores, or if they have opposite values with positively correlated attention maps."
  },
  {
    "objectID": "posts/weighted-linear-attention-is-lotka-volterra/index.html#conclusion",
    "href": "posts/weighted-linear-attention-is-lotka-volterra/index.html#conclusion",
    "title": "Weighted Linear Attention is Lotka-Volterra Dynamics",
    "section": "Conclusion",
    "text": "Conclusion\nAttention mechanisms have become the cornerstone of modern sequence models. However, because the standard formulation of attention scales with the length of the context window squared, there has been a lot of recent interest in alternative approaches with linear scaling. Most of these alternative architectures involve weighted linear attention modules of some form. So far, these architectures based on key-value associative memories have generally fallen short of their transformer counterparts, raising an important question of `why?’.\nHere, I have provided a new lens through which to view attention mechanisms–that of ecosystems. Specifically, I’ve shown that weighted associative memories based on query-key-value recall mechanisms correspond to ecological communities in which the tokens are species and the weights in the memory are their species abundances. In fact, this is more than a colorful metaphor, and the dynamics of exponentiated gradient descent on a squared reconstruction loss are exactly described by Lotka-Volterra dynamics.\nThis theory highlights some simple tools for understanding the inner workings of attention mechanisms. For example, the covariance between a token’s attention scores and its alignment scores determines its intrinsic growth rate. However, the intrinsic growth rate is, by itself, not enough to specify the weight of a token in the memory because tokens also interact with each other. In fact, the alignment of two tokens value vectors and the covariance of their attention scores determines if the tokens compete with each for space in the memory, or if they actually cooperate and reinforce each other’s weights.\nIn the following post, Weighted Linear Attention as Species Invasion, I will show that within a streaming context, online updating of an associative memory is equivalent to the invasion of an ecosystem by a new species. And, I’ll use this mapping to derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated linear attention."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog\nTechnology is often developed out in the open, through open source projects that allow one to follow along with the development process in addition to consuming the final product. But scientific research is usually done behind closed doors, for reasons I don’t entirely understand. The thinking process that leads to a paper isn’t shared, except among a few collaborators. This blog is my attempt at truly open scientific research.\nThe blog is called “Local Minimum” because many of the ideas I share here will be early and immature. It’s a nod to the way can systems can get stuck in a local minimum of an energy landscape.\n\n\nAbout the author\nCharles Fisher is typically interested in research at the intersection of physics, machine learning, and biology. He has a PhD in biophysics from Harvard University, a BS in biophysics from the University of Michigan, and did postdoctoral research in biophysics at Boston University and Ecole Normale Superieure in Paris. Following his time in academia, he moved to industry where he worked at Pfizer and Leap Motion before starting Unlearn.AI. He was CEO of Unlearn for nearly 8 years before stepping back to a board role so that he could get back to his roots and dive into research again. Hence, this blog.\n\n\nAcknowledgments\nI’m often discussing my research with Pankaj Mehta and his group at Boston University, Austin Huang, Anton Loukianov, and others."
  }
]