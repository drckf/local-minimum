---
title: "What is Weighted Linear Attention?"
author: "Charles Fisher"
date: "2025-04-06"
categories: [review, machine-learning, attention]
image: thumbnail.jpg
bibliography: weighted-linear-attention.bib
---

## TLDR

Weighted linear attention is a type of key-value associative memory defined by the matrix
$$
\begin{equation}
J = \sum_{l'} w_{l'} \, \vec{v}_{l'} \, \vec{k}_{l'}^T
\end{equation}
$$
and an associative recall formula $\tilde{v}_l = J \vec{q}_l$ that retrieves a value correpsonding to a query vector.
These modules are interesting components for neural sequence models because they can be viewed as alternatives to 
softmax attention that may be more efficient for long context windows. 


## Softmax Attention

The softmax attention module was a key innovation in the development of transformer models that are 
commonly used for natural language processing and other applications in machine learning [@vaswani2017attention]. 
At a high level, attention leverages the idea that the meaning of a word in a document could change depending 
on the other words in the document. Therefore, in order to determine the meaning of a given word (or 
a component of a word called a token) we need to compare it to the rest of the words in the document. 

Let $\vec{x}_l \in \mathbb{R}^n$ 
for $l = 1, \ldots, L$ be a sequence of tokens, $W_Q$ and $W_K$ be $d_k \times n$ matrices, and $W_V$
be a $d_v \times n$ matrix. The queries, keys, and values are given by
$$
\begin{align}
\vec{q}_l &= W_Q \, \vec{x}_l \in \mathbb{R}^{d_k} \\
\vec{k}_l &= W_K \, \vec{x}_l \in \mathbb{R}^{d_k} \\
\vec{v}_l &= W_V \, \vec{x}_l \in \mathbb{R}^{d_v}
\end{align}
$$
The softmax attention module defines an $L \times L$ matrix $\mathcal{A}$ with elements
$$
\mathcal{A}_{l,l'} = \frac{e^{ d_k^{-1/2} \vec{q}_l^T \vec{k}_{l'} }}{ \sum_{l''} e^{d_k^{-1/2} \vec{q}_l^T \vec{k}_{l''} }}
$$
that describes how query $l$ attends to key $l'$. Thus, the value retrived for query $l$ is
$$
\tilde{v}_l = \sum_{l'} \mathcal{A}_{l,l'} \vec{v}_{l'}
$$
Of course, there are many variations of this general module that have been introduced in the last few years, but
I'm not going to get into those because the original formulation has the necessary elements for this introduction. 

Softmax attention is relatively easy to implement in parallel because it is stateless. This means that it efficiently
utilizes the capabilities of modern GPUs, but it also requires a large amount of memory and computational resources
because it involves computing an $L \times L$ matrix. Thus, softmax attention uses resources that are quadratic in the 
length of the context window. Although one can use a variety of tricks to mitigate this problem, it nevertheless 
represents a fundamental limitation to the length of sequences that transformers are able to process. 


## Weighted Linear Attention

In the last few years, there has been renewed interest in stateful architectures
for modeling sequential data such as language or time series [@wang2025test; @aksenov2024linear; 
@schlag2021linear; @yang2025parallelizing; @katharopoulos2020transformers; @peng2021random; 
@beck2025xlstm; @qin2022cosformer; @kasai2021finetuning; @zhang2024hedgehog; @chen2024dijiang; 
@sun2023retentive; @orvieto2023resurrecting; @katsch2023gateloop; @de2024griffin; @qin2024hgrn2; 
@peng2024eagle; @yang2023gated; @gu2023mamba; @dao2024transformers; @liu2024longhorn; @sun2024learning; 
@yang2024gated; @behrouz2024titans]. Stateful models are a 
more memory and compute efficient alternative to stateless models like transformers because they scale 
linearly with the length of the sequence in contrast to the quadratic scaling of transformers. 
In principle, stateful language models could have much cheaper inference costs
and would be able to operate over essentially infinite context windows. 

Modern stateful sequence models typically use a key-value associative memory as 
an alternative to the softmax attention heads in transformers.  A key-value associative memory is a 
$d_v \times d_k$ matrix $J_l$ that is typically, but not necessarily, updated through a recurrent 
relation such as
$$
\begin{equation}
J_l = \omega_{f,l} \, J_{l-1} + \omega_{i,l} \, \vec{v}_l \, \vec{k}_l^T
\end{equation}
$$
where $\omega_{f,l} \geq 0$ is a forget gate and $\omega_{i,l} \geq 0$ is an input gate. 
The input and forget gates could be constants, functions of the current state, or could 
even be functions of all previous states in the sequence. A value is retrieved from the 
memory by multiplication with a query vector $\tilde{v}_l = J \, \vec{q}_l$, where I am
denoting the retrieved value with a tilde to emphasize that $\tilde{v}_i$ will generally 
not equal $\vec{v}_i$ unless the memory has perfect recall. See [Figure 1](#fig-linear-attention) 
below for examples of different linear attention mechanisms.

It's trivial to see that an associative memory matrix trained through this type of recurrent
update rule will take the form
$$
\begin{equation}
J = \sum_{l'} w_{l'} \, \vec{v}_{l'} \, \vec{k}_{l'}^T
\end{equation}
$$
where $w_{l'} \geq 0$ is the weight assigned to token $l'$ in the memory. I will refer to 
key-value associative memories with this structure as "weighted linear attention" modules. 

To illustrate the connection between the recurrent update and the weighted linear attention form, 
consider unrolling the recurrence. Starting with
$$
J_l = \omega_{f,l} J_{l-1} + \omega_{i,l}\, \vec{v}_l\, \vec{k}_l^T,
$$
recursively substituting $J_{l-1}$ gives
$$
J_l = \sum_{l'=1}^{l} \left( \prod_{l''=l'+1}^{l} \omega_{f,l''} \right) \omega_{i,l'}\, \vec{v}_{l'}\, \vec{k}_{l'}^T.
$$
This shows that the associative memory $J_l$ is a weighted sum of outer products of values and keys, 
where each token's contribution is scaled by a weight 
$w_{l'} = \left( \prod_{l''=l'+1}^{l} \omega_{f,l''} \right) \omega_{i,l'}$. 
These weights capture how much past tokens are retained in memory, clarifying how stateful, 
linear attention aggregates information over time.

Although much of the practical interest in linear attention architectures is the ability to
define them through recurrent update rules, it's not clear that these recurrent update rules
optimize their memory capacity. That is, there may be alternative ways to learn the weights
that lead to better performing associative memories, at least in theory. I'll explore this in 
a series of upcoming blog posts focused on a surprising connection between weighted linear 
attention modules and ecological systems. 

Specifically, I will show that weighted linear attention can be interpreted as an evolving ecological
system in which tokens are species and their weights are species abundances. The weights evolve under 
Lotka-Volterra dynamics when optimized via exponentiated gradient descent to minimize the squared recall error. 
This framework provides explicit formulas linking the statistics of the data distribution to ecological parameters 
such as the carrying capacity and interaction coefficients of each token. In a streaming context, online updating 
of an associative memory is equivalent to the invasion of an ecosystem by a new species. I use this mapping to 
derive some novel ecologically inspired attention modules, including a closed-form solution for optimal gated 
linear attention. Follow along with the next post in this series 
[Weighted Linear Attention is Lotka-Volterra Dynamics](/posts/weighted-linear-attention-is-lotka-volterra/).

---

The following table from Yang et al (2024) highlights a number of models that use various
types of linear attention modules and recurrent update rules:

![Types of Recurrent Updates for Linear Attention. (Source: @yang2024parallelizing)](neurips24_poster_deltanet.jpg){#fig-linear-attention}

---

::: {.license-footer}
Â© 2025 Charles Fisher. This work is licensed under a [Creative Commons Attribution 4.0 International License](/license.html).
::: 